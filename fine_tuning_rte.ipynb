{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Installing libraries"
      ],
      "metadata": {
        "id": "VIntji8wtERu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "azWttSZfrwOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing libraries"
      ],
      "metadata": {
        "id": "ceF81m80t0EX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
        "from tqdm import tqdm\n",
        "import csv"
      ],
      "metadata": {
        "id": "YvIcvMwLt2rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Code for fine-tuning\n",
        "Note: \"output.csv\" file must be present in the same directory as this \"fine_tuning_rte.ipynb\" file.\n",
        "\n",
        "Public link to output.csv: https://drive.google.com/file/d/1fm6SDn0TQckZqLFs7ctNODOZNPLC1W_z/view?usp=share_link"
      ],
      "metadata": {
        "id": "nYJkTgeDtSO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = []\n",
        "with open('output.csv', 'r', newline='', encoding='utf-8') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    for row in reader:\n",
        "        separator = \"|\"\n",
        "        evidence_list = [item for item in row['evidence'].split(separator)]\n",
        "        # print(evidence_list)\n",
        "        training_data.append({'claim': row['claim'], 'evidence': evidence_list, 'label': row['label']})\n",
        "\n",
        "# check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# load pre-trained model and tokenizer for textual entailment\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-large-mnli\").to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"roberta-large-mnli\")\n",
        "\n",
        "# encode training data\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "labels = []\n",
        "\n",
        "for data in tqdm(training_data[:1000]):\n",
        "    claim = data[\"claim\"]\n",
        "    evidence_sentences = data[\"evidence\"]\n",
        "    label = data[\"label\"]\n",
        "    \n",
        "    # join evidence sentences into a single string\n",
        "    evidence_text = \" \".join(evidence_sentences)\n",
        "    \n",
        "    # encode claim and evidence text into token IDs\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        claim,                      # claim text to encode\n",
        "                        evidence_text,              # evidence text to encode\n",
        "                        add_special_tokens = True,  # add [CLS] and [SEP] tokens\n",
        "                        max_length = 512,           # truncate/pad to this length\n",
        "                        padding = 'max_length',     # pad to max length\n",
        "                        return_attention_mask = True, # return attention masks\n",
        "                        return_tensors = 'pt'       # return PyTorch tensors\n",
        "                  )\n",
        "    \n",
        "    # get token IDs and attention mask from encoded dictionary\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "    \n",
        "    # convert label to numerical value\n",
        "    if label == \"REFUTES\":\n",
        "        labels.append(0)\n",
        "    elif label == \"SUPPORTS\":\n",
        "        labels.append(1)\n",
        "    else:\n",
        "        labels.append(2)\n",
        "\n",
        "# convert data to PyTorch tensors\n",
        "input_ids = torch.cat(input_ids, dim=0).to(device)\n",
        "attention_masks = torch.cat(attention_masks, dim=0).to(device)\n",
        "labels = torch.tensor(labels).to(device)\n",
        "\n",
        "# define batch size and create data loader\n",
        "batch_size = 2\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "sampler = SequentialSampler(dataset)\n",
        "dataloader = DataLoader(dataset, sampler=sampler, batch_size=batch_size)\n",
        "\n",
        "# fine-tune the model on training data\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "for epoch in range(3):\n",
        "    for batch in tqdm(dataloader):\n",
        "        input_ids = batch[0].to(device)\n",
        "        attention_masks = batch[1].to(device)\n",
        "        labels = batch[2].to(device)\n",
        "        \n",
        "        model.zero_grad()\n",
        "        \n",
        "        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n",
        "        \n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "# save the fine-tuned model\n",
        "model.save_pretrained(\"fine-tuned-roberta-large-mnli\")\n",
        "tokenizer.save_pretrained(\"fine-tuned-roberta-large-mnli\")"
      ],
      "metadata": {
        "id": "tj9TZM_6ryhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: If fine-tuning is done in Google Colab, the model saved in cloud has to be downloaded manually and placed in the same directory as \"test_rte.ipynb\" and \"test_custom_rte\" files, so that the fine-tuned model can be tested appropriately."
      ],
      "metadata": {
        "id": "D2KbDfqG432q"
      }
    }
  ]
}