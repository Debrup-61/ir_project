{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aad9ad9e",
   "metadata": {},
   "source": [
    "# Fact Verification and Evidence Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad88697",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe4790a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import os.path as path\n",
    "import heapq\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3762a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wikipedia in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (1.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from wikipedia) (4.12.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from wikipedia) (2.28.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2022.12.7)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from beautifulsoup4->wikipedia) (2.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7801bc7f",
   "metadata": {},
   "source": [
    "### Define the paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24ff6da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base dir: /Users/debrup/PycharmProjects/ir_project/ir_project\n",
      "Raw Training set: /Users/debrup/PycharmProjects/ir_project/ir_project/fever/train.jsonl\n",
      "Training DocRetrieval Output File: /Users/debrup/PycharmProjects/ir_project/ir_project/fever/train.wiki7.jsonl\n",
      "Database path: /Users/debrup/PycharmProjects/ir_project/ir_project/fever/fever.db\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = os.getcwd()\n",
    "\n",
    "raw_training_set = path.join(BASE_DIR, \"fever/train.jsonl\")\n",
    "training_doc_file = path.join(BASE_DIR,\"fever/train.wiki7.jsonl\")\n",
    "db_path = path.join(BASE_DIR, \"fever/fever.db\")\n",
    "\n",
    "print(\"Base dir:\",BASE_DIR)\n",
    "print(\"Raw Training set:\",raw_training_set)\n",
    "print(\"Training DocRetrieval Output File:\",training_doc_file)\n",
    "print(\"Database path:\",db_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca765dd",
   "metadata": {},
   "source": [
    "### Check if gpu is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b81d1db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66576020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.backends.mps.is_built())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd3c1430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.device(\"mps\") else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff85ba7",
   "metadata": {},
   "source": [
    "### Open the train.jsonl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1164eea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reader:\n",
    "    def __init__(self,encoding=\"utf-8\"):\n",
    "        self.enc = encoding\n",
    "\n",
    "    def read(self,file):\n",
    "        with open(file,\"r\",encoding = self.enc) as f:\n",
    "            return self.process(f)\n",
    "\n",
    "    def process(self,f):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a599e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JSONLineReader(Reader):\n",
    "    def process(self,fp):\n",
    "        data = []\n",
    "        for line in fp.readlines():\n",
    "            data.append(json.loads(line.strip()))\n",
    "        return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600bf40e",
   "metadata": {},
   "source": [
    "### Read lines of train.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70b1359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = dict()\n",
    "jlr = JSONLineReader()\n",
    "lines = jlr.read(raw_training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45e8fe27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481f1c0c",
   "metadata": {},
   "source": [
    "### Get the claim lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3509f296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.', 'Roman Atwood is a content creator.', 'History of art includes architecture, dance, sculpture, music, painting, poetry literature, theatre, narrative, film, photography and graphic arts.']\n",
      "[[[[92206, 104971, 'Nikolaj_Coster-Waldau', 7], [92206, 104971, 'Fox_Broadcasting_Company', 0]]], [[[174271, 187498, 'Roman_Atwood', 1]], [[174271, 187499, 'Roman_Atwood', 3]]], [[[255136, 254645, 'History_of_art', 2]]]]\n"
     ]
    }
   ],
   "source": [
    "claim_test_lines = [sub['claim'] for sub in lines]\n",
    "evidence_test_lines = [ sub['evidence'] for sub in lines]\n",
    "\n",
    "claim_test_lines = claim_test_lines[0:50]\n",
    "evidence_test_lines = evidence_test_lines[0:50]\n",
    "\n",
    "print(claim_test_lines[0:3])\n",
    "print(evidence_test_lines[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b974f8",
   "metadata": {},
   "source": [
    "### Install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8320b2d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stanza in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (1.5.0)\n",
      "Requirement already satisfied: emoji in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from stanza) (2.2.0)\n",
      "Requirement already satisfied: numpy in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from stanza) (1.24.2)\n",
      "Requirement already satisfied: protobuf in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from stanza) (4.22.1)\n",
      "Requirement already satisfied: requests in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from stanza) (2.28.2)\n",
      "Requirement already satisfied: six in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from stanza) (1.16.0)\n",
      "Requirement already satisfied: torch>=1.3.0 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from stanza) (2.0.0)\n",
      "Requirement already satisfied: tqdm in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from stanza) (4.65.0)\n",
      "Requirement already satisfied: filelock in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from torch>=1.3.0->stanza) (3.10.7)\n",
      "Requirement already satisfied: typing-extensions in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from torch>=1.3.0->stanza) (4.5.0)\n",
      "Requirement already satisfied: sympy in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from torch>=1.3.0->stanza) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from torch>=1.3.0->stanza) (3.0)\n",
      "Requirement already satisfied: jinja2 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from torch>=1.3.0->stanza) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from requests->stanza) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from requests->stanza) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from requests->stanza) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from requests->stanza) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install stanza\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc372d7c",
   "metadata": {},
   "source": [
    "### Import stanza and download english model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a27e7f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json: 200kB [00:03, 59.7kB/s]\n",
      "2023-04-12 21:57:26 INFO: Downloading default packages for language: en (English) ...\n",
      "2023-04-12 21:57:28 INFO: File exists: /Users/debrup/stanza_resources/en/default.zip\n",
      "2023-04-12 21:57:33 INFO: Finished downloading models and saved to /Users/debrup/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('en') # download the English model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ec6b9b",
   "metadata": {},
   "source": [
    "### Try NER on the claim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555d8169",
   "metadata": {},
   "source": [
    "### Install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbc316d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (3.5.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from spacy) (8.1.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from spacy) (1.24.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from spacy) (2.28.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from spacy) (1.10.7)\n",
      "Requirement already satisfied: jinja2 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from spacy) (67.6.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from jinja2->spacy) (2.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a9710a",
   "metadata": {},
   "source": [
    "### Try NER using spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95c3560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "# docs = list(nlp.pipe(claim_test_lines[0:3]))\n",
    "\n",
    "# for doc in docs:\n",
    "#     for ent in doc.ents:\n",
    "#         print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d8e56d",
   "metadata": {},
   "source": [
    "#### As we can see the NER Models cannot get all the required entities. They can only derive very specific entity objects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b98c51",
   "metadata": {},
   "source": [
    "### Install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c53cc046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from nltk) (2023.3.23)\n",
      "Requirement already satisfied: tqdm in /Users/debrup/miniconda3/envs/def/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6102a5",
   "metadata": {},
   "source": [
    "### Extract all the noun phrases of the claims using constituency tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f473337",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 21:57:37 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json: 200kB [00:04, 44.0kB/s]\n",
      "2023-04-12 21:57:44 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2023-04-12 21:57:44 INFO: Using device: cpu\n",
      "2023-04-12 21:57:44 INFO: Loading: tokenize\n",
      "2023-04-12 21:57:44 INFO: Loading: pos\n",
      "2023-04-12 21:57:44 INFO: Loading: constituency\n",
      "2023-04-12 21:57:44 INFO: Done loading processors!\n",
      " 64%|███████████████████████████▌               | 32/50 [03:37<01:42,  5.68s/it]"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "import wikipedia\n",
    "from tqdm import tqdm\n",
    "\n",
    "def preprocess(np):\n",
    "    page = np.replace('( ', '-LRB-')\n",
    "    page = page.replace(' )', '-RRB-')\n",
    "    page = page.replace(' - ', '-')\n",
    "    page = page.replace(' :', '-COLON-')\n",
    "    page = page.replace(' ,', ',')\n",
    "    page = page.replace(\" 's\", \"'s\")\n",
    "    page = page.replace(' ', '_')\n",
    "    return page\n",
    "    \n",
    "\n",
    "claim_train_lines = [ sub['claim'] for sub in lines]\n",
    "\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency',tokenize_pretokenized=True)\n",
    "entities=[]  # entities will be a list of size len(train_claims) having list of entities(noun_phrases) as each elem\n",
    "wiki_pages=[]\n",
    "\n",
    "\n",
    "\n",
    "for claim in tqdm(claim_test_lines):\n",
    "        doc = nlp(claim)\n",
    "        sentence=doc.sentences[0]\n",
    "        tree = sentence.constituency\n",
    "        np=[]\n",
    "        tree.visit_preorder(internal = lambda x: np.append(x.leaf_labels()) if (x.label==\"NP\" or x.label==\"NML\") else None)\n",
    "        noun_phrases = [' '.join(n) for n in np]\n",
    "        entities.append(noun_phrases)\n",
    "        #www = wikipedia.search(n)\n",
    "        #print(\"LLLL:\",len(noun_phrases))\n",
    "        #print(tree)\n",
    "            \n",
    "        #predicted_wiki = [preprocess((wikipedia.search(n))[0]) for n in noun_phrases] \n",
    "        predicted_wiki = []\n",
    "        for n in noun_phrases:\n",
    "            if not len(wikipedia.search(n)) == 0:\n",
    "                predicted_wiki.append(preprocess((wikipedia.search(n))[0]))\n",
    "        wiki_pages.append(predicted_wiki)\n",
    "        \n",
    "        ##print(noun_phrases)\n",
    "        ##print(type(sentence.constituency))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed37a5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39d9717",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dbe981",
   "metadata": {},
   "source": [
    "### DEBUG TO FIND EMPTY SUBLIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3286d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "for sublist in entities:\n",
    "    if not sublist:\n",
    "        print(\"Empty sublist found!\")\n",
    "        print()\n",
    "        break\n",
    "    c=c+1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3973a18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "for sublist in wiki_pages:\n",
    "    if not sublist:\n",
    "        print(\"Empty sublist found!\")\n",
    "        print()\n",
    "        break\n",
    "    c=c+1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895e293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "for sublist in claim_test_lines:\n",
    "    if not sublist:\n",
    "        print(\"Empty sublist found!\")\n",
    "        print()\n",
    "        break\n",
    "    c=c+1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047de893",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities[44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bd39c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_pages[44]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aa424d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wiki_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbb6a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for claim in tqdm(claim_test_lines[859:1000]):\n",
    "    \n",
    "#         doc = nlp(claim)\n",
    "#         sentence=doc.sentences[0]\n",
    "#         tree = sentence.constituency\n",
    "#         np=[]\n",
    "#         tree.visit_preorder(internal = lambda x: np.append(x.leaf_labels()) if (x.label==\"NP\" or x.label==\"NML\") else None)\n",
    "#         noun_phrases = [' '.join(n) for n in np]\n",
    "#         entities.append(noun_phrases)\n",
    "#         #www = wikipedia.search(n)\n",
    "#         #print(\"LLLL:\",len(noun_phrases))\n",
    "#         #print(tree)\n",
    "            \n",
    "#         #predicted_wiki = [preprocess((wikipedia.search(n))[0]) for n in noun_phrases] \n",
    "#         predicted_wiki = []\n",
    "#         for n in noun_phrases:\n",
    "#             if not len(wikipedia.search(n)) == 0:\n",
    "#                 predicted_wiki.append(preprocess((wikipedia.search(n))[0]))\n",
    "#         wiki_pages.append(predicted_wiki)\n",
    "        \n",
    "#         ##print(noun_phrases)\n",
    "#         ##print(type(sentence.constituency))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f57a2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wiki_pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5cdf4a",
   "metadata": {},
   "source": [
    "### Example noun phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d9fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06348771",
   "metadata": {},
   "source": [
    "### Example predicted wiki pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4ae6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4553a103",
   "metadata": {},
   "source": [
    "### Example code for retrieving from fever.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d03357",
   "metadata": {},
   "outputs": [],
   "source": [
    "P=preprocess('List_of_The_Simpsons_guest_stars_(seasons 21–present)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad63fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Open a connection to the database file\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Create a cursor object to execute SQL commands\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Execute a SELECT query on the database\n",
    "query = f\"SELECT lines FROM documents where id='Game_of_Thrones'\"\n",
    "cursor.execute(query)\n",
    "\n",
    "# Retrieve the results of the query\n",
    "results = cursor.fetchone()\n",
    "\n",
    "# Print the results\n",
    "for row in results:\n",
    "    print(row)\n",
    "\n",
    "# Close the connection to the database\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234996c2",
   "metadata": {},
   "source": [
    "### Get the names of titles and the lines of pages in fever.db that matches with wiki_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad00d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Open a connection to the database file\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Create a cursor object to execute SQL commands\n",
    "cursor = conn.cursor()\n",
    "\n",
    "candidate_lines = []  ## list of size (no_claims), each element of list should be lines of each matched page\n",
    "\n",
    "count=0\n",
    "\n",
    "for claim_pages in wiki_pages:\n",
    "    \n",
    "    \n",
    "    for page in claim_pages:\n",
    "        \n",
    "        #print(\"page:\",page)\n",
    "        #Execute a SELECT query on the database\n",
    "        query = \"SELECT lines FROM documents WHERE id=?\"\n",
    "        cursor.execute(query, (page,))\n",
    "\n",
    "        #print(\"query:\",query)\n",
    "       \n",
    "        # Retrieve the results of the query\n",
    "        results = cursor.fetchone()\n",
    "        \n",
    "        #print(results)\n",
    "        \n",
    "       # If results are empty continue\n",
    "        if results is None or len(results) == 0:\n",
    "            continue  \n",
    "            \n",
    "       # Split the string into a list of lines using the newline character\n",
    "        lines_page = results[0].split('\\n')   \n",
    "        #print(lines_page)\n",
    "        \n",
    "        for line in lines_page:\n",
    "            \n",
    "         \n",
    "           # remove tab characters\n",
    "           line = line.replace('\\t', ' ')\n",
    "           \n",
    "           while len(line) >= 1 and line[0].isdigit():\n",
    "                try:\n",
    "                    line = line[1:]\n",
    "                    \n",
    "                except:\n",
    "                    line=''\n",
    "                    break\n",
    "                \n",
    "           # remove digits\n",
    "           #line = line.translate(str.maketrans('', '', '0123456789'))\n",
    "            \n",
    "\n",
    "           # remove all words after last . (the links to other pages)\n",
    "           last_period_index = line.rfind(\".\")\n",
    "\n",
    "           # Remove everything after the last period\n",
    "           \n",
    "           if last_period_index != -1:\n",
    "               \n",
    "                line = line[:last_period_index+1]\n",
    "                   \n",
    "           \n",
    "        \n",
    "           #line = line.split('.')[0]\n",
    "    \n",
    "           # remove extra spaces\n",
    "           line = ' '.join(line.split())\n",
    "            \n",
    "           # Each of the elements of candidate lines is a dictionary with (title,lines_page) (K,V) pairs\n",
    "           \n",
    "         \n",
    "           try:\n",
    "            \n",
    "              ((candidate_lines[count])[page]).append(line)\n",
    "                    \n",
    "           except:\n",
    "              \n",
    "              if len(candidate_lines)==count:\n",
    "                 candidate_lines.append({})\n",
    "                    \n",
    "               \n",
    "                 \n",
    "            \n",
    "              try:\n",
    "                      ((candidate_lines[count])[page]).append(line)\n",
    "              except:\n",
    "                      ((candidate_lines[count])[page])=[]\n",
    "                      ((candidate_lines[count])[page]).append(line)  \n",
    "                    \n",
    "        \n",
    "        \n",
    "        \n",
    "    count=count+1\n",
    "\n",
    "\n",
    "print(candidate_lines[0])\n",
    "\n",
    "# Close the connection to the database\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f085789",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(candidate_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b788810c",
   "metadata": {},
   "source": [
    "### DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11600dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "for sublist in candidate_lines:\n",
    "    if not sublist:\n",
    "        print(\"Empty sublist found!\")\n",
    "        print()\n",
    "        break\n",
    "    c=c+1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e29d69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Open a connection to the database file\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Create a cursor object to execute SQL commands\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Define the SQL query with a placeholder for the id parameter\n",
    "query = \"SELECT lines FROM documents WHERE id=?\"\n",
    "\n",
    "# Execute the query with the id parameter passed as a tuple\n",
    "id = \"Robert_J._O'Neill\"\n",
    "cursor.execute(query, (id,))\n",
    "\n",
    "# Fetch the results and print the first row\n",
    "results = cursor.fetchone()\n",
    "print(results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3801a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(candidate_lines[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19827fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the retrieved documents for each claim\n",
    "\n",
    "retrieved_docs = []\n",
    "for i in candidate_lines:\n",
    "    retrieved_docs.append(list(i.keys()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247c2495",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0197db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce71918",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Find list of ground truth documents\n",
    "evidence_test_lines[0]\n",
    "\n",
    "ground_truth_docs=[]\n",
    "for i in range(len(evidence_test_lines)):\n",
    "    l1 = evidence_test_lines[i]\n",
    "    ground_truth_docs.append([])\n",
    "    for j in range(len(l1)):\n",
    "        for l2 in evidence_test_lines[i][j]:\n",
    "             ground_truth_docs[i].append(l2[2])\n",
    "                \n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8660b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ground_truth_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7952aa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find document retrieval accuracy\n",
    "\n",
    "total_docs=0\n",
    "correct_docs=0\n",
    "for i in range(len(ground_truth_docs)):\n",
    "    \n",
    "    claim_docs=ground_truth_docs[i]\n",
    "    for doc in claim_docs:\n",
    "        total_docs=total_docs+1\n",
    "        print(doc)\n",
    "        if doc in ground_truth_docs[i]:\n",
    "            correct_docs=correct_docs+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a41288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find document retrieval accuracy\n",
    "\n",
    "total_docs=0\n",
    "correct_docs=0\n",
    "for i in range(len(ground_truth_docs)):\n",
    "    \n",
    "    ground_truth_doc=ground_truth_docs[i]\n",
    "    total_docs=total_docs+1\n",
    "    \n",
    "    if ground_truth_doc==[None]:\n",
    "        correct_docs=correct_docs+1 \n",
    "        #print(\"Here \",i)\n",
    "        continue\n",
    "        \n",
    "    flag=1\n",
    "    \n",
    "    #print(\"GG\", retrieved_docs[i])\n",
    "    #print(ground_truth_doc)\n",
    "    for doc in ground_truth_doc:\n",
    "        if doc not in retrieved_docs[i]:\n",
    "            flag=0\n",
    "            #print(\"HERE\", i)\n",
    "            break\n",
    "            \n",
    "    if flag==1:\n",
    "        correct_docs=correct_docs+1 \n",
    "    \n",
    "   \n",
    "       \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a357e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955c1057",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_docs=0\n",
    "correct_docs=0\n",
    "\n",
    "# Accuracy of document retrieval for each claim \n",
    "# Correct if the entire ground truth for a claim  is a subset of the retrieved document list for a claim\n",
    "for i in range(len(ground_truth_docs)):\n",
    "    doc = ground_truth_docs[i]\n",
    "    total_docs=total_docs+1\n",
    "    \n",
    "    if len(doc) == 1 and doc[0] is None:\n",
    "        print(\"YES\")\n",
    "        correct_docs=correct_docs+1\n",
    "        print(\"a\", retrieved_docs[i])\n",
    "        print(\"b\", doc)\n",
    "        \n",
    "    elif set(doc).issubset(set(retrieved_docs[i])):\n",
    "        print(\"YES\")\n",
    "        correct_docs=correct_docs+1\n",
    "        print(\"a\", retrieved_docs[i])\n",
    "        print(\"b\", doc)\n",
    "    else:\n",
    "        print(\"NO\")\n",
    "        print(\"a\", retrieved_docs[i])\n",
    "        print(\"b\", doc)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b039d168",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50d3deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca13b7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = correct_docs/total_docs\n",
    "print(\"Accuracy of document retrieval\",accuracy*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4a4dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2c5750",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ground_truth_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc14da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_test_lines[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12b2564",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(candidate_lines[1]['Roman_Atwood'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10463070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "# Query text\n",
    "query = \"List_of_The_Simpsons_guest_stars_(seasons 21–present)\"\n",
    "\n",
    "# Search for matching page titles\n",
    "results = wikipedia.search(query)\n",
    "\n",
    "# Print the list of matching page titles\n",
    "print(\"Matching page titles:\")\n",
    "for title in results:\n",
    "    print(title)\n",
    "\n",
    "# Choose a page to retrieve\n",
    "page_title = results[0]\n",
    "\n",
    "# Retrieve the content of the page\n",
    "page = wikipedia.page(page_title)\n",
    "\n",
    "# Print the page summary and content\n",
    "print(\"Page summary:\", page.summary)\n",
    "##print(\"Page content:\", page.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ca7556",
   "metadata": {},
   "source": [
    "### Compute Similarity between the Claim and the Candidate Evidences found using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0ebe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Open a connection to the database file\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Create a cursor object to execute SQL commands\n",
    "cursor = conn.cursor()\n",
    "\n",
    "query = \"SELECT lines FROM documents\"\n",
    "cursor.execute(query)\n",
    "\n",
    "results = cursor.fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5464364",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = len(results)\n",
    "documents = []\n",
    "for i in range(0, 300):\n",
    "    document = results[i][0]\n",
    "    \n",
    "    # remove tab characters\n",
    "    document = document.replace('\\t', ' ')\n",
    "    \n",
    "    # remove all words after last . (the links to other pages)\n",
    "    last_period_index = document.rfind(\".\")\n",
    "\n",
    "   # Remove everything after the last period\n",
    "\n",
    "    if last_period_index != -1:\n",
    "        document = document[:last_period_index+1]\n",
    "    \n",
    "#     while len(document) >= 1 and document[0].isdigit():\n",
    "#         try:\n",
    "#             document = document[1:]\n",
    "                    \n",
    "#         except:\n",
    "#             document=''\n",
    "#             break\n",
    "\n",
    "    document = ' '.join(document.split())\n",
    "    \n",
    "    documents.append(document)\n",
    "    \n",
    "print((documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8da9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bbb9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22aa232",
   "metadata": {},
   "source": [
    "### Term frequency and inverse document frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc10dbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b3f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(claim_test_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55d7c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(claim_test_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8762777",
   "metadata": {},
   "source": [
    "### Extract all sentences with relevance scores  as tuples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e970ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Vectorize the claim and evidences\n",
    "vectors = vectorizer.fit_transform(documents)\n",
    "# candidate_sentence = candidate_lines[0]\n",
    "\n",
    "# Candidate_lines[i] -> the collection of candidate lines in an ith claim from all retrieved documents\n",
    "\n",
    "\n",
    "claim_similarities = []\n",
    "for i in range(len(claim_test_lines)):\n",
    "    similarity_scores_list = []\n",
    "    sim_score = {}\n",
    "    claim = claim_test_lines[i]\n",
    "    \n",
    "    max_similarity_score = 0\n",
    "    max_candidate_sentence = \"\"\n",
    "    \n",
    "    for key in candidate_lines[i].keys():\n",
    "        similarity_all_sentences = []\n",
    "        candidate_list_sentence = candidate_lines[i][key]\n",
    "        \n",
    "        for idx in range(len(candidate_list_sentence)) :\n",
    "            \n",
    "            candidate_sentence = candidate_list_sentence[idx]\n",
    "           # Vectorize the new sentences\n",
    "            new_vectors = vectorizer.transform([claim,candidate_sentence])\n",
    "\n",
    "           # Calculate cosine similarity between the two new sentences\n",
    "            similarity_scores = cosine_similarity(new_vectors)\n",
    "            \n",
    "            max_similarity_score = max(max_similarity_score, similarity_scores[0][1])\n",
    "            if(similarity_scores[0][1] == max_similarity_score):\n",
    "                max_candidate_sentence = candidate_sentence\n",
    "            \n",
    "            similarity_scores_list.append((key,idx, candidate_sentence, similarity_scores[0][1]))\n",
    "        \n",
    "    claim_similarities.append(similarity_scores_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ada642",
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bfa43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(claim_similarities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a496d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(claim_similarities[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fac585",
   "metadata": {},
   "source": [
    "### Extract top K evidence sentences for every claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2f101e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_list = [(1, 2, 3, 4), (5, 6, 7, 8), (9, 10, 11, 2), (13, 14, 15, 8), (17, 18, 19, 1)]\n",
    "\n",
    "def topK(my_list, K):\n",
    "    # Find the indices of the top K tuples in the list based on the 4th element\n",
    "    top_indices = heapq.nlargest(K, range(len(my_list)), key=lambda i: my_list[i][3])\n",
    "\n",
    "    # Find the tuples corresponding to the top K indices\n",
    "    top_tuples = [my_list[i] for i in top_indices]\n",
    "\n",
    "    # Print the top indices and tuples\n",
    "    \n",
    "    zipped_list = list(zip(top_indices, top_tuples))\n",
    "    \n",
    "    return zipped_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf44c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "K=5\n",
    "topK_evidences_for_each_claim = []\n",
    "for i in range(len(claim_similarities)):\n",
    "    topK_evidences_for_each_claim.append(topK(claim_similarities[i], K))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df459a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topK_evidences_for_each_claim\n",
    "# \n",
    "# Inner tuple:\n",
    "# \n",
    "\n",
    "# 1st index: position in the claim_no\n",
    "\n",
    "#     inner tuple:\n",
    "#         1st element: retrieved document name \n",
    "#         2nd: index of the candidate sentence in the retrieved document\n",
    "#         3rd: candidate sentence\n",
    "#         4th: Similarity score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4fc21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(topK_evidences_for_each_claim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae693f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((topK_evidences_for_each_claim)[0][0][1][0])  #i j 1 0\n",
    "print((topK_evidences_for_each_claim)[0][0][1][1])  #i j 1 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fbe75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((topK_evidences_for_each_claim)[0][0])  #i j 1 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333f53dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_evidences_tfidf=[]\n",
    "\n",
    "for i in range(len(topK_evidences_for_each_claim)):\n",
    "    retrieved_evidences_tfidf.append([])\n",
    "    t = topK_evidences_for_each_claim[i]\n",
    "    for j in range(len(t)):\n",
    "         retrieved_evidences_tfidf[i].append([topK_evidences_for_each_claim[i][j][1][0],topK_evidences_for_each_claim[i][j][1][1]])\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc8663",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(retrieved_evidences_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb302c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_evidences_tfidf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad2bcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_test_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302cf32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Evidence retrieval accuracy\n",
    "\n",
    "# for i in range(len(evidence_test_lines)):\n",
    "#     evidence_sets = evidence_test_lines[i]\n",
    "#     print(\"rrr\",retrieved_evidences_tfidf[i])\n",
    "#     for j in range(len(evidence_sets)):\n",
    "#         evidence_set = evidence_sets[j]\n",
    "#         print(\"eee\",evidence_set) \n",
    "#         for k in range(len(evidence_set)):\n",
    "#             evidence_set[k]=(evidence_set[k])[2:]\n",
    "            \n",
    "#         if set(evidence_set).issubset(set(retrieved_evidences_tfidf[i])):\n",
    "#             print(\"True\")\n",
    "          \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a620fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_test_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2069c329",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_test_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179b39fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_similarities[0][11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f67df0",
   "metadata": {},
   "source": [
    "### Find relevance of each candidate evidence sentence using BERT-based models(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb4fd59",
   "metadata": {},
   "source": [
    "### Install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956f07c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68c7a9c",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa79d418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686c6d54",
   "metadata": {},
   "source": [
    "### Load pre-trained BERT model and tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d9a843",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "### This is a sequence classification task with two labels (Relevant or not)\n",
    "#model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32a31ed",
   "metadata": {},
   "source": [
    "### Get (claim,evidence,label) pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35534ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(evidence_test_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b27ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evidence_test_lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2560e8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evidence_test_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b2f4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Open a connection to the database file\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Create a cursor object to execute SQL commands\n",
    "cursor = conn.cursor()\n",
    "\n",
    "claims_to_train_on = [ sub['claim'] for sub in lines]\n",
    "claims_to_train_on =claims_to_train_on[0:50]        # We take 50 claims to get examples\n",
    "\n",
    "claim_label_pos = []\n",
    "candidate_evidence_pos = []\n",
    "claim_pos = []\n",
    "\n",
    "for i in range(len(claims_to_train_on)):    # For all the claims\n",
    "    \n",
    "    for j in range(len(evidence_test_lines[i][0])):\n",
    "        \n",
    "            page = evidence_test_lines[i][0][j][2]\n",
    "            \n",
    "            #Check if page is NONE (can happen in not enough info claim case)\n",
    "            if (page==None):  # For None type page (for not enough info)\n",
    "                continue\n",
    "            \n",
    "\n",
    "            line_no = evidence_test_lines[i][0][j][3]\n",
    "            \n",
    "           \n",
    "            # Execute a SELECT query on the database\n",
    "            query = \"SELECT lines FROM documents WHERE id=?\"\n",
    "            cursor.execute(query, (page,))\n",
    "            \n",
    "            # Retrieve the results of the query\n",
    "            results = cursor.fetchone()\n",
    "                \n",
    "            lines_page = results[0].split('\\n') \n",
    "            line = lines_page[line_no]\n",
    "            line = line.replace('\\t', ' ')\n",
    "            \n",
    "            while len(line) >= 1 and line[0].isdigit():\n",
    "                try:\n",
    "                    line = line[1:]\n",
    "                    \n",
    "                except:\n",
    "                    line=''\n",
    "                    break\n",
    "                \n",
    "      \n",
    "            # remove all words after last . (the links to other pages)\n",
    "            last_period_index = line.rfind(\".\")\n",
    "            \n",
    "            # Remove everything after the last period\n",
    "            if last_period_index != -1:\n",
    "                line = line[:last_period_index+1]\n",
    "                   \n",
    "           \n",
    "            # remove extra spaces\n",
    "            line = ' '.join(line.split())\n",
    "            \n",
    "            claim_pos.append(claims_to_train_on[i])\n",
    "            candidate_evidence_pos.append(line)\n",
    "            claim_label_pos.append(1)\n",
    "            \n",
    "                \n",
    "            \n",
    "# Close the connection to the database\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1544ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_label_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124d0db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_evidence_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6614c1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4236c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(claim_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc60a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(claim_label_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef5e406",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(candidate_evidence_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ad4fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "for sublist in claim_pos:\n",
    "    if not sublist:\n",
    "        print(\"Empty sublist found!\")\n",
    "        print()\n",
    "        break\n",
    "    c=c+1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ce1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "for sublist in claim_label_pos:\n",
    "    if not sublist:\n",
    "        print(\"Empty sublist found!\")\n",
    "        print()\n",
    "        break\n",
    "    c=c+1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445c6ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "for sublist in candidate_evidence_pos:\n",
    "    if not sublist:\n",
    "        print(\"Empty sublist found!\")\n",
    "        print()\n",
    "        break\n",
    "    c=c+1   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f793d650",
   "metadata": {},
   "source": [
    "### Negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8205fe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(claims_to_train_on))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba6452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_neg = 1\n",
    "k=10\n",
    "\n",
    "claim_neg = []\n",
    "candidate_neg = []\n",
    "label_neg =[]\n",
    "\n",
    "import re\n",
    "\n",
    "def check_digit(s):\n",
    "    # remove any extra spaces or tabs from the string\n",
    "    s = s.strip()\n",
    "    # define a regular expression pattern to match 2-digit numbers\n",
    "    pattern = r'^\\d{1,2}$'\n",
    "    # use the re.match() function to check if the string matches the pattern\n",
    "    match = re.match(pattern, s)\n",
    "    # if the match object is not None, then the string is a 2-digit number\n",
    "    if match:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(claims_to_train_on)):\n",
    "    claim = claims_to_train_on[i]\n",
    "    # Look at pages related to claim\n",
    "    random_pages = wikipedia.search(claim)\n",
    "    #print(\"AAAAAAAAA........\",random_pages)\n",
    "        \n",
    "    n = len(random_pages)\n",
    "    #print(\"value of n\",n)\n",
    "        \n",
    "    # Check if n=0 then continue(no search result for the claim)\n",
    "    if(n==0):\n",
    "       continue\n",
    "            \n",
    "    for j in range(no_of_neg):\n",
    "        \n",
    "        # Pick a random page from the list\n",
    "        page_name = random_pages[random.randint(0, n-1)]\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(\"BBBBBBB\",page_name)\n",
    "       \n",
    "        \n",
    "        conn = sqlite3.connect(db_path)\n",
    "\n",
    "        # Create a cursor object to execute SQL commands\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        page_name=preprocess(page_name)\n",
    "        \n",
    "        # Execute a SELECT query on the database\n",
    "        query = \"SELECT lines FROM documents WHERE id=?\"\n",
    "        cursor.execute(query, (page_name,))\n",
    "        \n",
    "        \n",
    "        # Retrieve the results of the query\n",
    "        results = cursor.fetchone()\n",
    "        \n",
    "        #print(\"CCCCCCC\",results)\n",
    "        \n",
    "        # If results are empty continue\n",
    "        if results is None or len(results) == 0:\n",
    "            continue  \n",
    "            \n",
    "        # Split the string into a list of lines using the newline character\n",
    "        lines_page = results[0].split('\\n') \n",
    "        #lines_page = lines_page[0:k]\n",
    "        #print(type(lines_page))\n",
    "        \n",
    "        # Get a random line from first k lines\n",
    "        flag = True\n",
    "\n",
    "        # simulate a do-while loop\n",
    "       \n",
    "         \n",
    "        # sample random line\n",
    "        line = lines_page[random.randrange(len(lines_page))]\n",
    "\n",
    "        # preprocess the line\n",
    "        line = line.replace('\\t', ' ')\n",
    "\n",
    "        while len(line) >= 1 and line[0].isdigit():\n",
    "            try:\n",
    "                line = line[1:]\n",
    "\n",
    "            except:\n",
    "                line=''\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "        # remove all words after last . (the links to other pages)\n",
    "        last_period_index = line.rfind(\".\")\n",
    "\n",
    "        # Remove everything after the last period\n",
    "\n",
    "        if last_period_index != -1:\n",
    "            line = line[:last_period_index+1]\n",
    "\n",
    "        # remove extra spaces\n",
    "        line = ' '.join(line.split())\n",
    "\n",
    "        if len(line)==0:\n",
    "           continue \n",
    "\n",
    "        \n",
    "        #print((claim,line,0))                             \n",
    "        claim_neg.append(claim)\n",
    "        candidate_neg.append(line)\n",
    "        label_neg.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5102d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(claim_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b332cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(candidate_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f13f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(label_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f6cde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(claim_neg[1])\n",
    "print(candidate_neg[1])\n",
    "print(label_neg[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37443ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "for sublist in claim_neg:\n",
    "    if not sublist:\n",
    "        print(\"Empty sublist found!\")\n",
    "        print()\n",
    "        break\n",
    "    c=c+1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef7bd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "for sublist in candidate_neg:\n",
    "    if not sublist:\n",
    "        print(\"Empty sublist found!\")\n",
    "        print()\n",
    "        break\n",
    "    c=c+1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a800465",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(claim_pos[0]) \n",
    "print(candidate_evidence_pos[0])\n",
    "print(claim_label_pos[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df4218e",
   "metadata": {},
   "source": [
    "### Define train_claims,train_evidence and train_labels using positive and negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc724fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_claims = claim_pos + claim_neg\n",
    "train_evidence = candidate_evidence_pos + candidate_neg\n",
    "train_labels = claim_label_pos + label_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517a5e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fdc428",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_claims))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fa15bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_evidence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50567d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1b70da",
   "metadata": {},
   "source": [
    "### Load your training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61cc356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize input sequences\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "for claim, evidence in zip(train_claims, train_evidence):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        claim,\n",
    "                        evidence,\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 256,\n",
    "                        padding = 'max_length',\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',\n",
    "                   )\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed81747",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07425c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94df521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec00f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(attention_masks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9957557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f82e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e79d788",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348ee37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d50e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939cb095",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e237461b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a2f32e",
   "metadata": {},
   "source": [
    "### Construct dataset with (input_ids,attention_masks,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feb426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine input sequences with labels\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91b45c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37ba7d0",
   "metadata": {},
   "source": [
    "### Construct the dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38559e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders for batching\n",
    "\n",
    "batch_size = 4\n",
    "train_dataloader = DataLoader(train_dataset, sampler=torch.utils.data.RandomSampler(train_dataset), batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, sampler=torch.utils.data.SequentialSampler(val_dataset), batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84830a93",
   "metadata": {},
   "source": [
    "### Load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9a101d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00ebfeb",
   "metadata": {},
   "source": [
    "### Finetune the pretrained BERT Model using training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf28bcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the optimizer and loss function\n",
    "import torch.nn.functional as F\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 5\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print('Training epoch %d...' % epoch)\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_masks = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n",
    "        #print(\"AAAAA\",outputs[0])\n",
    "        loss = criterion(outputs[1], labels)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "    print('Average training loss: %.4f' % (total_loss / len(train_dataloader)))\n",
    "\n",
    "# Evaluate the fine-tuned model on your validation set\n",
    "# Let's assume your validation set is in the same format as your training set\n",
    "# and you have converted it into a DataLoader of input tensors called val_dataloader\n",
    "# with the same structure as train_dataloader\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_loss = 0\n",
    "    val_accuracy = 0\n",
    "    val_steps = 0\n",
    "    \n",
    "    for batch in val_dataloader:\n",
    "        input_ids = batch[0].to(device)\n",
    "        attention_masks = batch[1].to(device)\n",
    "        labels = batch[2].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n",
    "        \n",
    "        # Extract loss and logits from the model output\n",
    "        val_loss += outputs.loss.item()\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Convert logits to probabilities and get predicted labels\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "        # Compute accuracy for the batch\n",
    "        val_accuracy += torch.sum(preds == labels).item()\n",
    "\n",
    "        # Update the number of evaluation steps\n",
    "        val_steps += 1\n",
    "        \n",
    "        \n",
    "# Calculate average evaluation loss and accuracy\n",
    "avg_val_loss = val_loss / val_steps\n",
    "avg_val_accuracy = val_accuracy / len(val_dataset)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abf3dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dc7e5c",
   "metadata": {},
   "source": [
    "### Print Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe4385f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"avg val accuracy\", avg_val_accuracy)\n",
    "print(\"avg loss\",avg_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af694a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the sentences with top relevant scores for every claim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14742b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(candidate_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b41cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(claim_test_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e02c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(claim_similarities_bert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7497df",
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_test_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910cbc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_similarities_bert=[]\n",
    "\n",
    "for i in tqdm(range(len(claim_test_lines))):\n",
    "    \n",
    "    similarity_scores_list_bert = []\n",
    "    sim_score_bert = {}\n",
    "    claim = claim_test_lines[i]\n",
    "    \n",
    "    max_similarity_score_bert = 0\n",
    "    max_candidate_sentence_bert = \"\"\n",
    "    \n",
    "    for key in candidate_lines[i].keys():\n",
    "        similarity_all_sentences = []\n",
    "        candidate_list_sentence = candidate_lines[i][key]\n",
    "        \n",
    "        for idx in range(len(candidate_list_sentence)) :\n",
    "            \n",
    "            candidate_sentence = candidate_list_sentence[idx]\n",
    "            \n",
    "            #Vectorize the new sentences\n",
    "            #new_vectors = vectorizer.transform([claim,candidate_sentence])\n",
    "\n",
    "            #Calculate cosine similarity between the two new sentences\n",
    "            #similarity_scores = cosine_similarity(new_vectors)\n",
    "            \n",
    "            ## Tokenizer acts on (claim,sentence)\n",
    "            encoded_dict = tokenizer.encode_plus(\n",
    "                        claim,\n",
    "                        candidate_sentence,\n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 256,\n",
    "                        padding = 'max_length',\n",
    "                        return_attention_mask = True,\n",
    "                        return_tensors = 'pt',\n",
    "                   )\n",
    "            input_ids = encoded_dict['input_ids'].to(device)\n",
    "            attention_mask = encoded_dict['attention_mask'].to(device)\n",
    "            \n",
    "            #print(\"Input_ids shape\",input_ids.shape)\n",
    "            #print(\"Att masks shape\",attention_mask.shape)\n",
    "            \n",
    "            ## Get the output corresponding to this example\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            probs = F.softmax(outputs.logits, dim=1)\n",
    "            probs_relevance = probs[0,1]\n",
    "            \n",
    "            max_similarity_score_bert = max(max_similarity_score_bert, probs_relevance)\n",
    "            if(probs_relevance == max_similarity_score_bert):\n",
    "                max_candidate_sentence_bert = candidate_sentence\n",
    "            \n",
    "            similarity_scores_list_bert.append((key,idx, candidate_sentence,probs_relevance))\n",
    "        \n",
    "    claim_similarities_bert.append(similarity_scores_list_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a9104",
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_similarities_bert[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdffd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_similarities_bert[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93805ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d381b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the top k corresponding to each claim(Same as tf-idf)\n",
    "\n",
    "topK_evidences_for_each_claim_bert = []\n",
    "for i in range(len(claim_similarities_bert)):\n",
    "    topK_evidences_for_each_claim_bert.append(topK(claim_similarities_bert[i], K))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d359a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "topK_evidences_for_each_claim_bert[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ede8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the bert model used for evidence retrieval\n",
    "\n",
    "# Save the model to a file\n",
    "model.save_pretrained('models/')\n",
    "\n",
    "# Save the tokenizer to a file\n",
    "tokenizer.save_pretrained('models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61997c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model from a file\n",
    "model_load = BertForSequenceClassification.from_pretrained('models/')\n",
    "\n",
    "# Load the saved tokenizer from a file\n",
    "tokenizer_load = BertTokenizer.from_pretrained('models/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe5f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d15565",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aafe51",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train_lines = [ sub['label'] for sub in test_lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff14edce",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train_lines = label_train_lines[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f8342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_train_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffffe0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of dictionaries \n",
    "\n",
    "retrieved_evidence=[]\n",
    "\n",
    "for i in range(len(claim_test_lines)):\n",
    "    retrieved_evidence.append({})\n",
    "    \n",
    "    (retrieved_evidence[i])['claim']=claim_test_lines[i]\n",
    "    (retrieved_evidence[i])['evidence']=[]\n",
    "    ret_evidence =topK_evidences_for_each_claim_bert[i]\n",
    "    (retrieved_evidence[i])['label']= label_train_lines[i]\n",
    "    for j in range(len(ret_evidence)):\n",
    "        (retrieved_evidence[i])['evidence'].append(ret_evidence[j][1][2])\n",
    "        \n",
    "    \n",
    "print(retrieved_evidence)    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1fc78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of dictionaries \n",
    "\n",
    "retrieved_evidence_new=[]\n",
    "\n",
    "for i in range(len(claim_test_lines)):\n",
    "    retrieved_evidence_new.append({})\n",
    "    \n",
    "    (retrieved_evidence_new[i])['claim']=claim_test_lines[i]\n",
    "    (retrieved_evidence_new[i])['evidence']=[]\n",
    "    ret_evidence =topK_evidences_for_each_claim_bert[i]\n",
    "    (retrieved_evidence_new[i])['label']= label_train_lines[i]\n",
    "    \n",
    "    for j in range(len(ret_evidence)):\n",
    "        (retrieved_evidence_new[i])['evidence'].append(ret_evidence[j][1][2])\n",
    "        \n",
    "    \n",
    "    separator = \"|\"\n",
    "    joint_evidence = separator.join(item for item in (retrieved_evidence_new[i])['evidence'])\n",
    "    (retrieved_evidence_new[i])['string_evidence']=joint_evidence\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "print(retrieved_evidence_new)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a07707",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(retrieved_evidence))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a287919",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_evidence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6bbb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_evidence_new[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43e53ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make CSV file with columns claim,evidence,label\n",
    "\n",
    "import csv\n",
    "\n",
    "# Example list of dictionaries\n",
    "\n",
    "# Open a CSV file for writing\n",
    "with open('retrieved_evidence_output1.csv', 'w', newline='') as csvfile:\n",
    "    \n",
    "    # Define the fieldnames for the CSV file\n",
    "    fieldnames = ['claim', 'evidence', 'label','string_evidence']\n",
    "    \n",
    "    # Create a writer object and write the fieldnames to the CSV file\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Write each row of data to the CSV file\n",
    "    for row in retrieved_evidence_new:\n",
    "        writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34a2f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_evidences_bert=[]\n",
    "\n",
    "for i in range(len(topK_evidences_for_each_claim_bert)):\n",
    "    retrieved_evidences_bert.append([])\n",
    "    t = topK_evidences_for_each_claim_bert[i]\n",
    "    for j in range(len(t)):\n",
    "         retrieved_evidences_bert[i].append([topK_evidences_for_each_claim_bert[i][j][1][0],topK_evidences_for_each_claim_bert[i][j][1][1]])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838ecf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_evidences_bert[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473e3156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "def_env",
   "language": "python",
   "name": "def_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
