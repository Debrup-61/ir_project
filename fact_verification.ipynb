{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Connecting to Google Drive where the pre-trained model is stored"
      ],
      "metadata": {
        "id": "RmLbexr5DGZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "%cd gdrive/MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxulB4GLDDo_",
        "outputId": "388874c4-fc9c-4971-d544-313f72c2eb9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n",
            "/content/gdrive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installing libraries"
      ],
      "metadata": {
        "id": "-FMbr1JD5qYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "X57T6-jO5t-2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75df6976-b919-40f7-86d9-22e2fd8e05e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.27.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Importing libraries"
      ],
      "metadata": {
        "id": "CKaXViOA5wQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, SequentialSampler\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import csv\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "RzbZy5r36KT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Defining the predict_textual_entailment() function\n",
        "Note: The folder \"fine-tuned-roberta-large-mnli\" must be at the same level as this file for the code to function properly.\n",
        "\n",
        "Public link to the pre-trained fine-tuned model: https://drive.google.com/drive/folders/1ShykW5wmMt2bWRx9AjdO36KUJBie4vkX?usp=sharing"
      ],
      "metadata": {
        "id": "60S19iF16Lj6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0nhIMgH5TO_"
      },
      "outputs": [],
      "source": [
        "# load fine-tuned model and tokenizer\n",
        "model_name = \"fine-tuned-roberta-large-mnli\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "def predict_textual_entailment(claim, evidence_sentences):\n",
        "    # join evidence sentences into a single string\n",
        "    evidence_text = \" \".join(evidence_sentences)\n",
        "    \n",
        "    # encode claim and evidence text into token IDs\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        claim,                      # claim text to encode\n",
        "                        evidence_text,              # evidence text to encode\n",
        "                        add_special_tokens = True,  # add [CLS] and [SEP] tokens\n",
        "                        max_length = 512,           # truncate/pad to this length\n",
        "                        padding = 'max_length',     # pad to max length\n",
        "                        return_attention_mask = True, # return attention masks\n",
        "                        return_tensors = 'pt'       # return PyTorch tensors\n",
        "                   )\n",
        "    \n",
        "    # get token IDs and attention mask from encoded dictionary\n",
        "    input_ids = encoded_dict['input_ids']\n",
        "    attention_mask = encoded_dict['attention_mask']\n",
        "    \n",
        "    # predict the textual entailment label using the pre-trained model\n",
        "    with torch.no_grad():\n",
        "        logits = model(input_ids, attention_mask=attention_mask)\n",
        "        entailment_probabilities = torch.softmax(logits[0], dim=1).tolist()[0]\n",
        "        entailment_labels = [\"REFUTES\", \"SUPPORTS\", \"NOT ENOUGH INFO\"]\n",
        "        predicted_textual_entailment = entailment_labels[entailment_probabilities.index(max(entailment_probabilities))]\n",
        "    \n",
        "    return predicted_textual_entailment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing the model with custom claim-evidence pair"
      ],
      "metadata": {
        "id": "5uw2g28k59F2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "claim = \"Manoj is taller than Suraj.\"\n",
        "evidence = [\n",
        "    \"Suraj is taller than Purohit.\", \"Purohit is taller than Manoj.\"\n",
        "]\n",
        "\n",
        "# The correct label is REFUTES\n",
        "\n",
        "\n",
        "print(\"Claim: \", claim)\n",
        "print(\"Evidences: \")\n",
        "cnt = 1\n",
        "for evid in evidence:\n",
        "  print(cnt, \": \", evid)\n",
        "  cnt += 1\n",
        "predicted_label = predict_textual_entailment(claim, evidence)\n",
        "print(\"Label: \", predicted_label)"
      ],
      "metadata": {
        "id": "_ECJ9bwR5iBW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98d6fdb3-599a-4e73-d166-3ef4827d05bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Claim:  Manoj is taller than Suraj.\n",
            "Evidences: \n",
            "1 :  Suraj is taller than Purohit.\n",
            "2 :  Purohit is taller than Manoj.\n",
            "Label:  REFUTES\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing the model on the development dataset\n",
        "\n",
        "Public link to the development dataset \"output.csv\": https://drive.google.com/file/d/1fm6SDn0TQckZqLFs7ctNODOZNPLC1W_z/view?usp=sharing"
      ],
      "metadata": {
        "id": "HpeRWDOQjrv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dev_data = []\n",
        "with open('output.csv', 'r', newline='', encoding='utf-8') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    for row in reader:\n",
        "        separator = \"|\"\n",
        "        evidence_list = [item for item in row['evidence'].split(separator)]\n",
        "        # print(evidence_list)\n",
        "        dev_data.append({'claim': row['claim'], 'evidence': evidence_list, 'label': row['label']})"
      ],
      "metadata": {
        "id": "ZhY6R0utgcxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_label = []\n",
        "ground_label = []\n",
        "for row in tqdm(dev_data[1000:1100]):\n",
        "  predicted_label.append(predict_textual_entailment(row['claim'], row['evidence']))\n",
        "  ground_label.append(row['label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69u-wU6A2gu7",
        "outputId": "ec329e46-627a-4029-f3be-c130b0c1d150"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [09:48<00:00,  5.89s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(ground_label, predicted_label)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmPlZyno2qRu",
        "outputId": "48a838d3-8763-468b-f5fd-8a233f2285c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Individual Class accuracy analysis"
      ],
      "metadata": {
        "id": "YxE4cK1tJNtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class1_pred = []\n",
        "class1_ground = []\n",
        "class2_pred = []\n",
        "class2_ground = []\n",
        "class3_pred = []\n",
        "class3_ground = []\n",
        "for ind in range(len(ground_label)):\n",
        "  label = ground_label[ind]\n",
        "  pred = predicted_label[ind]\n",
        "  if label == 'SUPPORTS':\n",
        "    class1_ground.append(label)\n",
        "    class1_pred.append(pred)\n",
        "  elif label == 'REFUTES':\n",
        "    class2_ground.append(label)\n",
        "    class2_pred.append(pred)\n",
        "  else:\n",
        "    class3_ground.append(label)\n",
        "    class3_pred.append(pred)\n",
        "accuracy = accuracy_score(class1_pred, class1_ground)\n",
        "print(\"Supports Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(class2_pred, class2_ground)\n",
        "print(\"Refutes Accuracy:\", accuracy)\n",
        "accuracy = accuracy_score(class3_pred, class3_ground)\n",
        "print(\"NEI Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxPBdYGp3OsT",
        "outputId": "3be144a3-8db5-4752-c48e-3d2a08ae36aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supports Accuracy: 0.9166666666666666\n",
            "Refutes Accuracy: 0.5555555555555556\n",
            "NEI Accuracy: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weighted_f1 = f1_score(ground_label, predicted_label, average='weighted')\n",
        "print(weighted_f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8H0FVhCoUEe",
        "outputId": "38bc1524-3168-4909-c0c8-00fb0e90fe2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8212838915470495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Testing on unseen data"
      ],
      "metadata": {
        "id": "_Kjolsz7I8q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = []\n",
        "with open('retrieved_evidence_output1.csv', 'r', newline='', encoding='utf-8') as csvfile:\n",
        "    reader = csv.DictReader(csvfile)\n",
        "    for row in reader:\n",
        "        separator = \"|\"\n",
        "        evidence_list = [item for item in row['string_evidence'].split(separator)]\n",
        "        # print(evidence_list)\n",
        "        test_data.append({'claim': row['claim'], 'evidence': evidence_list, 'label': row['label']})"
      ],
      "metadata": {
        "id": "a92SYahyK5VP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_label = []\n",
        "ground_label = []\n",
        "for row in tqdm(test_data[:30]):\n",
        "  predicted_label.append(predict_textual_entailment(row['claim'], row['evidence']))\n",
        "  ground_label.append(row['label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywLcwvKV3Fe6",
        "outputId": "f9825772-d04d-4848-aa96-fd8369a2fabd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [02:57<00:00,  5.93s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(ground_label, predicted_label)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsQwhL9eLICO",
        "outputId": "a25657cc-0cba-4724-aa2e-4e7210cc6e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.43333333333333335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "sample = dev_data[1000]\n",
        "claim = sample['claim']\n",
        "evidence = sample['evidence']\n",
        "print(\"Claim: \", claim)\n",
        "print(\"Evidences: \")\n",
        "cnt = 1\n",
        "for evid in evidence:\n",
        "  wrapped = textwrap.fill(evid, width=80)\n",
        "  print(cnt, \": \", wrapped)\n",
        "  cnt += 1\n",
        "predicted_label = predict_textual_entailment(claim, evidence)\n",
        "print(\"Label: \", predicted_label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3jiF2ZiLL4v",
        "outputId": "7f8dd92b-297d-47be-86d3-208cd81cc16d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Claim:  One of the leads in Transformers: Age of Extinction is an American rapper.\n",
            "Evidences: \n",
            "1 :  It stars Mark Wahlberg , with Peter Cullen reprising his role as the voice of\n",
            "Optimus Prime , as the lead roles .\n",
            "2 :  Mark Robert Michael Wahlberg -LRB- born June 5 , 1971 -RRB- is an American actor\n",
            ", producer , businessman , former model , and rapper .\n",
            "Label:  SUPPORTS\n"
          ]
        }
      ]
    }
  ]
}